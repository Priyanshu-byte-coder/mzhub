---
title: "Ethical AI and Guardrail Framework Implementation for FaithTech Services"
description: "Comprehensive AI Governance Framework (RAIOps) to mitigate ethical and safety risks in FaithTech applications."
date: "2024-12-10"
author: "MZHub Team"
category: "AI Ethics"
tags: ["Responsible AI", "AI Governance", "Content Safety", "Azure", "Ethics"]
image: "/blog/blog10.jpg"
readTime: "14 min read"
---

# Ethical AI and Guardrail Framework Implementation for FaithTech Services

## 1\. Executive Summary: The Principle of Digital Faith Trust

### 1.1 Project Overview: Establishing Trust through Governance

This proposal outlines the deployment of a comprehensive, auditable AI Governance Framework (RAIOps) designed specifically to mitigate the unique ethical and safety risks inherent in FaithTech applications. The core objective is to move beyond conventional content filtering to establish proactive controls against religious bias, doctrinal inaccuracy (hallucination), and the erosion of user trust through unfair moderation.<sup>1</sup> The proposed framework integrates four critical layers of oversight, built entirely on Microsoft Azure infrastructure: Content Moderation Filters, Unified Cloud Governance (Policy Tools), Responsible AI Tooling (MLOps), and a Human-in-the-Loop review system. This holistic infrastructure is designed to translate abstract ethical commitments-such as fairness, reliability, and accountability <sup>3</sup>-into measurable, automated operational processes. Successful deployment will transition MZHub into a model of compliant FaithTech innovation, ready to operate as a **Significant Data Fiduciary (SDF)** under Indian law.<sup>5</sup>

### 1.2 Strategic Alignment for Dual Funding Success

The architectural decisions are strategically aligned to meet the core requirements of both potential funding partners. The system is fundamentally anchored in Microsoft's six Responsible AI principles (Fairness, Reliability and Safety, Privacy and Security, Transparency, Accountability, and Inclusiveness).<sup>3</sup> The \$150,000 Azure credit request is specifically justified by the need to fund the specialized Azure services-including Microsoft Purview, Azure AI Content Safety, and Azure Machine Learning Responsible AI Dashboard-required to operationalize these principles at the necessary scale and rigor. Concurrently, the framework demonstrates explicit and immediate readiness for compliance with the mandatory obligations of India's Digital Personal Data Protection (DPDP) Act (2023) <sup>5</sup> and adheres to the ethical compass set forth by the India AI Governance Guidelines (2025), which prioritize Trust, Accountability, and human-centric design.<sup>4</sup> By focusing the technical spend on risk mitigation and governance infrastructure, the project directly supports the Government of India's pillar for "Safe and Trusted AI".<sup>9</sup>

## 2\. The FaithTech Ethical Imperative (Problem & Solution)

The application of generative AI to religious and theological content presents heightened ethical and operational risks that surpass those found in general-purpose AI applications.

### 2.1 The Crisis of Bias, Misrepresentation, and Censorship

Standard large language models (LLMs), trained on culturally skewed internet data, exhibit pervasive religious and cultural biases. Analysis has shown that certain faith traditions may be negatively associated with terms like "bigot" or "homophobic," while minority or non-European religions often receive narrow or distorted representations.<sup>2</sup> This systemic bias risks algorithmic discrimination and unequal treatment, fundamentally violating the principle of fairness required by both Microsoft and the GoI.<sup>3</sup>

Furthermore, content moderation in FaithTech faces a severe dilemma: the challenge of balancing necessary hate speech detection against legitimate theological debate. Hate speech is defined as language that attacks or incites violence based on specific characteristics, including religion.<sup>11</sup> However, the complex, often highly polarized nature of religious discussions means that overly aggressive filtering-or "over-moderation"-can suppress political dissent or legitimate speech, leading to censorship and a critical erosion of user trust.<sup>1</sup> The core solution requires a nuanced content moderation layer that can employ contextual analysis to distinguish between protected debate and prohibited hate speech, ensuring transparency and accountability in the moderation process.<sup>12</sup>

### 2.2 Mitigating Theological Hallucination and Doctrinal Risk

A second critical risk is the tendency of generative models to produce inaccurate or entirely fabricated content, known as "hallucinations".<sup>2</sup> In a religious context, where doctrinal accuracy is crucial, even minor inaccuracies can generate misunderstandings, social tensions, or delegitimization.<sup>2</sup> Therefore, reliability and safety in FaithTech require technical grounding, moving beyond preventing harmful language to ensuring factual, theological correctness.

The proposed solution mandates that all generative AI services utilize the Retrieval-Augmented Generation (RAG) pattern, grounded in MZHub's vetted, proprietary theological knowledge base.<sup>14</sup> This technical choice transforms RAG from a performance enhancement tool into a critical **ethical guardrail** against doctrinal hallucination. This RAG architecture is then fortified by integrating **Groundedness Detection** capabilities, ensuring that every AI response presented as factual is logically supported by the source documents retrieved from the vector store.<sup>15</sup>

## 3\. Technical Architecture: The Azure Responsible AI Stack

The architectural design establishes a secure, compliant, and auditable MLOps environment by integrating specialized, mandated Azure services across three distinct governance layers, strictly adhering to MLOps best practices for security, access control, and auditing.<sup>16</sup>

### 3.1 Layer 1: Content Moderation Filters and Real-time Guardrails

The foundation of the framework is built on **Azure AI Content Safety**, which serves as the real-time, intelligent filtering system for all data ingress and egress.<sup>15</sup>

#### 3.1.1 Core Functionality and FaithTech Customization

The service detects and blocks harmful inputs and outputs across standard categories (violence, hate, sexual, self-harm).<sup>15</sup> For FaithTech applications, this service is customized to address unique threats:

- **Prompt Shield:** Activated universally to defend the LLM against jailbreaking and prompt injection attempts, thereby preventing the misuse of the system for generating malicious content.<sup>18</sup>
- **Custom AI Filters:** Unique content filters are created to specifically flag religiously sensitive terms or topics prone to polarization.<sup>13</sup> This flagging triggers an internal contextual human review loop <sup>20</sup>, minimizing the risk of over-moderation and censorship that standard filtering might cause.<sup>12</sup>
- **Groundedness Detection:** Integrated directly with the RAG architecture (utilizing Azure AI Search <sup>14</sup>), this feature ensures that content generated by the LLM is demonstrably accurate and rooted in the protected enterprise data, thereby mitigating theological hallucination.<sup>15</sup>
- **PII Masking:** PII detection is employed within the content filtering system to mask or filter sensitive Personal Identifiable Information (PII) and religious affiliation data before it is processed by the LLM, maintaining strict privacy and adhering to DPDP requirements.<sup>6</sup>

### 3.2 Layer 2: Cloud Governance and Data Privacy Policy Tools

Regulatory and data governance requirements, particularly those imposed by the DPDP Act for Significant Data Fiduciaries, are managed by **Microsoft Purview**.<sup>23</sup>

Purview's Data Map and Unified Catalog continuously scan and classify all user data and grounding data, identifying sensitive religious metadata and PII.<sup>23</sup> This provides the foundational visibility and data confidence necessary for conducting the mandated Data Protection Impact Assessments (DPIAs).<sup>5</sup> Furthermore, **Azure Policy** is utilized to enforce mandatory security and compliance baselines across the cloud environment, including securing ML workspaces <sup>25</sup> and enforcing the encryption of all sensitive data both at rest and in transit.<sup>17</sup> This integrated governance stack ensures that the protection of user data is not discretionary but is technically enforced through automated policies, supporting the DPDP Act's requirement for transparency regarding data collection, use, and storage.<sup>6</sup>

### 3.3 Layer 3: Responsible AI Tooling for Audit and Mitigation

The operationalization of the Fairness and Transparency principles occurs within the **Azure Machine Learning Studio** <sup>26</sup>, utilizing dedicated responsible AI tools.

The **Responsible AI Dashboard** provides a unified interface for rigorous model assessment, including data exploration, error analysis, and fairness evaluation.<sup>27</sup> To actively mitigate the inherited religious bias identified in Section 2.1, the **Fairlearn** open-source package is deployed within the Azure ML MLOps pipeline.<sup>28</sup> Fairlearn enables the assessment of quantitative fairness metrics, such as Demographic Parity and Equalized Odds.<sup>29</sup> The framework employs **reduction** algorithms (e.g., re-weighting training datasets based on sensitive attributes like religious affiliation) to actively reduce disparities in model outcomes.<sup>30</sup>

To ensure transparency and accountability, **InterpretML/SHAP** explainers are integrated to provide model interpretability.<sup>31</sup> This capability is crucial for generating auditable, human-readable justifications for high-risk decisions, directly supporting the compliance requirements for explainability under India's AI Governance Guidelines.<sup>4</sup>

**Azure Content Safety Implementation Parameters for FaithTech**

| **Moderation Category** | **Detection Threshold** | **Specific FaithTech Risk** | **Azure Service/Mitigation Strategy** |
| --- | --- | --- | --- |
| Hate & Discrimination | Custom Severity (High) | Religious/Sectarian Conflict, Targeted Identity Attacks, Extremism. | Azure AI Content Safety (Custom Filters) + Contextual Human Review.<sup>13</sup> |
| --- | --- | --- | --- |
| Prompt Shield Integrity | Critical/Block | LLM Jailbreak, unauthorized data exfiltration. | Azure AI Content Safety Prompt Shield activated on all ingress.<sup>19</sup> |
| --- | --- | --- | --- |
| Doctrinal Inaccuracy | Medium/Flag | Theological Hallucination (Content fabrication).<sup>2</sup> | Azure AI Content Safety Groundedness Detection + Azure AI Search RAG.<sup>14</sup> |
| --- | --- | --- | --- |
| Privacy Exposure (PII) | Critical/Mask | Leaking sensitive user PII or faith metadata during conversation history.<sup>6</sup> | PII masking applied in Azure AI Content Safety/Purview prior to LLM processing.<sup>21</sup> |
| --- | --- | --- | --- |

## 4\. Implementation Roadmap and MLOps Integration

The implementation roadmap is structured across four phases, prioritizing the establishment of governance and compliance infrastructure before scaling the deployment, thereby minimizing systemic risk.

### 4.1 Phase 1: Policy and Governance Readiness (Months 1-3)

The focus is defining the compliance baseline. Milestones include the formal adoption of the MZHub Responsible AI Standard, mapping it explicitly to Microsoft RAI and India's governance principles. Crucially, this phase includes appointing a dedicated Data Protection Officer (DPO) and initiating the mandatory Data Protection Impact Assessment (DPIA) process as required for a Significant Data Fiduciary under the DPDP Act.<sup>5</sup>

### 4.2 Phase 2: Tooling Integration and Pilot Deployment (Months 4-9)

This phase involves operationalizing the core cloud stack. Key technical milestones include deploying segregated Azure ML workspaces and enforcing secure configuration baselines via Azure Policy.<sup>17</sup> The Microsoft Purview integration must be completed, including the continuous scanning and classification of all sensitive data assets.<sup>23</sup> Azure AI Content Safety is integrated as the primary real-time guardrail layer, with Prompt Shield and initial custom moderation filters activated.<sup>15</sup>

### 4.3 Phase 3: Validation, Mitigation, and Human Oversight (Months 10-18)

The objective is to achieve the baseline targets for ethical metrics. The Responsible AI Dashboard is leveraged to assess fairness metrics across diverse user segments, followed by the application of Fairlearn mitigation techniques (re-weighting and post-processing) until the Demographic Parity Index (DPI) meets the pre-defined fairness threshold.<sup>27</sup> Concurrently, InterpretML is integrated to generate explanations for high-risk decisions, validating the Explainability Coverage KPI.<sup>32</sup> The Human-in-the-Loop (HITL) review dashboard is formalized, establishing the user appeal process and beginning the measurement of the Successful Appeal Rate (SAR).<sup>34</sup>

### 4.4 Phase 4: Scaling, Continuous Governance, and Audit Readiness (Months 19+)

The final phase focuses on achieving continuous enterprise-level governance and ensuring audit readiness. Automated monitoring systems are implemented to detect bias and data drift in production, triggering necessary re-training or human alerts.<sup>29</sup> The first mandatory annual security audit is conducted, and the DPIA submission is finalized, completing the regulatory requirement for the Significant Data Fiduciary.<sup>5</sup> This disciplined approach, which integrates security standards and risk management from the outset, directly aligns with the GoI's strategic focus on responsible AI deployment.<sup>9</sup>

**Technical Milestones for GoI Compliance**

| **Regulatory Mandate/Phase** | **Technical Milestone Deliverable** | **Compliance Impact** | **Timeline** |
| --- | --- | --- | --- |
| DPDP Act (SDF) Compliance <sup>5</sup> | Completion and submission of mandatory Data Protection Impact Assessment (DPIA). | Mitigate financial liability risk; secure operational license. | Phase 4 (19+ Months) |
| --- | --- | --- | --- |
| Safety & Reporting <sup>7</sup> | Real-time security monitoring enabled with 72-hour breach notification process established. | Meets legal requirement for rapid breach reporting to the Data Protection Board. | Phase 2 (9 Months) |
| --- | --- | --- | --- |
| Fairness & Equity <sup>4</sup> | Achieved Demographic Parity Index (DPI) ≤ 5% via Fairlearn deployment. | Proves commitment to non-discriminatory treatment across faith groups. | Phase 3 (18 Months) |
| --- | --- | --- | --- |
| Accountability (Audit) <sup>35</sup> | 100% of production models registered in the Model Catalog with full audit trails. | Enables fast, comprehensive audit readiness for GoI regulators. | Continuous (Phase 2+) |
| --- | --- | --- | --- |

## 5\. Measurable Impact and KPIs

Success is defined by the integrity of the process and the fairness of outcomes, quantified by auditable ethical metrics. These Key Performance Indicators (KPIs) measure the platform's adherence to compliance standards, human-centric design, and risk mitigation.

### 5.1 Fairness and Bias Mitigation KPIs

The **Demographic Parity Index (DPI)** is the primary metric, quantifying the ratio of positive outcomes across sensitive attributes (faith, cultural origin).<sup>29</sup> The goal is to achieve minimal performance disparity (e.g., less than 5% disparity) to prove success in mitigating systemic religious bias. The **Bias Drift Detection Rate** tracks the frequency of alerts indicating ethical performance degradation in production, necessitating continuous monitoring.<sup>35</sup>

### 5.2 Transparency and Accountability KPIs

**Explainability Coverage** measures the proportion of high-risk decisions-such as content removal or specific theological recommendations-that are logged with an auditable, human-readable justification generated by InterpretML.<sup>32</sup> This is essential for transparency and satisfying user appeal requirements.<sup>33</sup>

The **Successful Appeal Rate (SAR) / Overturn Rate** is a crucial measure of procedural justice, tracking the percentage of moderation decisions (both AI and human) that are reversed upon user appeal.<sup>34</sup> A consistently low SAR (Target: <1.0%) demonstrates that the moderation policies are clear, consistently applied, and not susceptible to arbitrary over-moderation, thereby maintaining user trust and proving accountability.<sup>12</sup>

### 5.3 Operational Safety and Compliance KPIs

The **Unsafe Content Filtering Rate** tracks the efficacy of Azure AI Content Safety in blocking prohibited content at the ingress and egress points.<sup>15</sup> Finally, the **Audit Readiness Score** monitors the ongoing organizational maturity by measuring the proportion of production models with up-to-date documentation, versioning, and finalized DPIAs, ensuring continuous compliance with regulatory mandates.<sup>5</sup>

**Responsible AI Governance KPI Matrix**

| **Pillar/Goal** | **Key Performance Indicator (KPI)** | **Metric Definition** | **Target Threshold** |
| --- | --- | --- | --- |
| **Fairness** | Demographic Parity Index (DPI) | Ratio of positive outcomes across sensitive faith/cultural groups.<sup>29</sup> | Disparity ≤ 5% |
| --- | --- | --- | --- |
| **Safety/Trust** | Unsafe Content Detection Rate | Volume of harmful content flagged by Content Safety before output.<sup>34</sup> | ≥ 99.9% |
| --- | --- | --- | --- |
| **Accountability** | Successful Appeal Rate (SAR) | Percentage of user appeals resulting in the reversal of a moderation decision.<sup>34</sup> | SAR ≤ 1.0% |
| --- | --- | --- | --- |
| **Compliance** | Audit Readiness Score | Proportion of production models with validated DPIAs and version control.<sup>5</sup> | 100% |
| --- | --- | --- | --- |
| **Transparency** | Explainability Coverage | Percentage of high-risk decisions with auditable justifications.<sup>32</sup> | ≥ 95% |
| --- | --- | --- | --- |

## 6\. Conclusions and Justification for Funding

The implementation of this Ethical AI and Guardrail Framework is not merely a technical deployment but a strategic imperative to manage the substantial financial and reputational risks associated with high-stakes FaithTech services.

The architectural commitment to Azure services directly addresses the dual regulatory landscape. By leveraging **Microsoft Purview** and **Azure Policy**, the framework enforces data security and access controls necessary to meet the stringent obligations of a Significant Data Fiduciary under the DPDP Act.<sup>5</sup> Failure to implement such robust security safeguards could lead to regulatory penalties reaching up to Rs 250 crore.<sup>7</sup> Therefore, the request for \$150,000 in Azure credits is fundamentally a request for **risk insurance and compliance infrastructure**, funding the specialized governance tools required to minimize this financial liability.

The framework further aligns with the strategic vision of the Government of India, particularly the focus on Safe and Trusted AI, by emphasizing homegrown solutions for bias mitigation, explainability, and governance testing.<sup>9</sup> The use of advanced Responsible AI Tooling (Fairlearn and InterpretML) and robust Content Moderation Filters (Azure AI Content Safety) ensures that MZHub's services adhere to the national principles of fairness, accountability, and human-centric design, positioning the startup as a trustworthy contributor to India's inclusive AI ecosystem.<sup>4</sup> This comprehensive governance architecture is essential for developing scalable, trustworthy, and compliant FaithTech solutions globally.

#### Works cited

- What are the ethical challenges of large-scale content moderation? - Tencent Cloud, accessed December 7, 2025, <https://www.tencentcloud.com/techpedia/121676>
- THE RISK OF RELIGIOUS BIAS IN ARTIFICIAL INTELLIGENCE - Mauro Cofelice - ETHICA SOCIETAS-Rivista di scienze umane e sociali, accessed December 7, 2025, <https://www.ethicasocietas.it/risk-of-religious-bias-in-artificial-intelligence/>
- Responsible AI: Ethical policies and practices | Microsoft AI, accessed December 7, 2025, <https://www.microsoft.com/en-us/ai/responsible-ai>
- AI for India, Built on Trust: What the New AI Governance Guidelines Mean for the Future, accessed December 7, 2025, <https://www.appknox.com/blog/india-ai-governance-guidelines-trust-security-appknox>
- Decoding the Digital Personal Data Protection Act, 2023 - EY, accessed December 7, 2025, <https://www.ey.com/en_in/insights/cybersecurity/decoding-the-digital-personal-data-protection-act-2023>
- What is Responsible AI - Azure Machine Learning | Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai?view=azureml-api-2>
- New data law spurs huge spike in cyber insurance demand, accessed December 7, 2025, <https://m.economictimes.com/tech/technology/new-data-law-spurs-huge-spike-in-cyber-insurance-demand/articleshow/125724690.cms>
- India Releases National AI Governance Guidelines to Balance Innovation and Risk, accessed December 7, 2025, <https://www.cdomagazine.tech/aiml/india-releases-national-ai-governance-guidelines-to-balance-innovation-and-risk>
- Transforming India with AI - Press Release:Press Information Bureau, accessed December 7, 2025, <https://www.pib.gov.in/PressReleasePage.aspx?PRID=2178092>
- Unmasking the Biases Within AI: How Gender, Ethnicity, Religion, and Economics Shape NLP and Beyond - Pacific AI, accessed December 7, 2025, <https://pacific.ai/unmasking-the-biases-within-ai-how-gender-ethnicity-religion-and-economics/>
- Hate speech detection: Challenges and solutions | PLOS One, accessed December 7, 2025, <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0221152>
- Ethical Consideration in AI Content Moderation : Avoiding Censorship and Biais - Checkstep, accessed December 7, 2025, <https://www.checkstep.com/ethical-consideration-in-ai-content-moderation-avoiding-censorship-and-biais>
- HaSpeeDe3 at EVALITA 2023: Overview of the Political and Religious Hate Speech Detection task - CEUR-WS.org, accessed December 7, 2025, <https://ceur-ws.org/Vol-3473/paper22.pdf>
- Retrieval Augmented Generation (RAG) in Azure AI Search - Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview>
- Content Safety in Foundry Control Plane | Microsoft Azure, accessed December 7, 2025, <https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety>
- Enterprise security and governance - Azure Machine Learning | Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/machine-learning/concept-enterprise-security?view=azureml-api-2>
- Architecture Best Practices for Azure Machine Learning - Microsoft Azure Well-Architected Framework, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/well-architected/service-guides/azure-machine-learning>
- Keeping Your AI Safe: Content Filters in Azure AI Foundry, accessed December 7, 2025, <https://monowar-mukul.medium.com/keeping-your-ai-safe-content-filters-in-azure-ai-foundry-9a87c8447e11>
- How to Bypass Azure AI Content Safety Guardrails - Mindgard, accessed December 7, 2025, <https://mindgard.ai/blog/bypassing-azure-ai-content-safety-guardrails>
- How does content moderation review content related to religion? - Tencent Cloud, accessed December 7, 2025, <https://www.tencentcloud.com/techpedia/122009>
- Azure OpenAI in Microsoft Foundry Models content filtering, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter?view=foundry-classic>
- Responsible AI in Azure Workloads - Microsoft Azure Well-Architected Framework, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/well-architected/ai/responsible-ai>
- Learn about data governance with Microsoft Purview, accessed December 7, 2025, <https://learn.microsoft.com/en-us/purview/data-governance-overview>
- Microsoft Purview Data Governance | Microsoft Security, accessed December 7, 2025, <https://www.microsoft.com/en-us/security/business/risk-management/microsoft-purview-data-governance>
- Azure security best practices and patterns - Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/security/fundamentals/best-practices-and-patterns>
- AI Architecture Design - Azure Architecture Center | Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/architecture/ai-ml/>
- Assess AI Systems and Make Data-Driven Decisions with Azure Machine Learning Responsible AI Dashboard - Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/machine-learning/concept-responsible-ai-dashboard?view=azureml-api-2>
- fairlearn/fairlearn: A Python package to assess and improve fairness of machine learning models. - GitHub, accessed December 7, 2025, <https://github.com/fairlearn/fairlearn>
- AI Evaluation Metrics - Bias & Fairness - FRANKI T, accessed December 7, 2025, <https://www.francescatabor.com/articles/2025/7/10/ai-evaluation-metrics-bias-amp-fairness>
- Machine learning fairness - Azure - Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml?view=azureml-api-2>
- azureml.interpret package - Azure Machine Learning Python | Microsoft Learn, accessed December 7, 2025, <https://learn.microsoft.com/en-us/python/api/azureml-interpret/azureml.interpret?view=azure-ml-py>
- InterpretML, accessed December 7, 2025, <https://interpret.ml/>
- Fairness metrics for health AI: we have a long way to go - PMC - NIH, accessed December 7, 2025, <https://pmc.ncbi.nlm.nih.gov/articles/PMC10114188/>
- Metrics for Content Moderation - Trust & Safety Professional Association, accessed December 7, 2025, <https://www.tspa.org/curriculum/ts-fundamentals/content-moderation-and-operations/metrics-for-content-moderation/>
- Key performance indicators (KPIs) for AI governance - VerifyWise AI Lexicon, accessed December 7, 2025, <https://verifywise.ai/lexicon/key-performance-indicators-kpis-for-ai-governance>
- Trust and safety metrics: How to measure digital trust of mobile apps - Incognia, accessed December 7, 2025, <https://www.incognia.com/the-authentication-reference/trust-and-safety-metrics>